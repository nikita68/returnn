#!crnn/rnn.py
# kate: syntax python;


#
# Based on Parnia Bahar's HMM factorization paper. For the attention we average the last attention layer over the
# heads. Note, we should try different approaches here in the future.
#

import os
from Util import get_login_username
demo_name, _ = os.path.splitext(__file__)
print("Hello, experiment: %s" % demo_name)

# task
use_tensorflow = True
task = "train"
#task = "search"

# data
#train = {"class": "TaskEpisodicCopyDataset", "num_seqs": 100}
#num_inputs = 10
#num_outputs = 10
train = {"class": "TaskXmlModelingDataset", "num_seqs": 100}
num_inputs = 12
num_outputs = 12

dev = train.copy()
dev.update({"num_seqs": train["num_seqs"] // 10, "fixed_random_seed": 42})
search_data = dev.copy()

# HMM Factorization data
vocab_size = 12
intermediary_size = 10
beam_size = 12

network = {
"source_embed": {"class": "linear", "activation": None, "with_bias": False, "n_out": 512},

"lstm0_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 10, "direction": 1, "from": ["source_embed"] , "dropout": 0.3},
"lstm0_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 10, "direction": -1, "from": ["source_embed"], "dropout": 0.3},

"lstm1_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 10, "direction": 1, "from": ["lstm0_fw", "lstm0_bw"] , "dropout": 0.3},
"lstm1_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 10, "direction": -1, "from": ["lstm0_fw", "lstm0_bw"], "dropout": 0.3},

"lstm2_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 10, "direction": 1, "from": ["lstm1_fw", "lstm1_bw"] , "dropout": 0.3},
"lstm2_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 10, "direction": -1, "from": ["lstm1_fw", "lstm1_bw"] , "dropout": 0.3},

"lstm3_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 10, "direction": 1, "from": ["lstm2_fw", "lstm2_bw"] , "dropout": 0.3},
"lstm3_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 10, "direction": -1, "from": ["lstm2_fw", "lstm2_bw"] , "dropout": 0.3},

"encoder": {"class": "copy", "from": ["lstm3_fw", "lstm3_bw"]},
"enc_ctx": {"class": "linear", "activation": None, "with_bias": True, "from": ["encoder"], "n_out": 10},  # preprocessed_attended in Blocks
"inv_fertility": {"class": "linear", "activation": "sigmoid", "with_bias": False, "from": ["encoder"], "n_out": 1},

"output": {"class": "rec", "from": [], "unit": {

    'output': {'class': 'choice', 'target': 'classes', 'beam_size': beam_size, 'from': ["output_prob"], "initial_output": 0},
    "end": {"class": "compare", "from": ["output"], "value": 0},
    'target_embed': {'class': 'linear', 'activation': None, "with_bias": False, 'from': ['output'], "n_out": 512, "initial_output": 0},
    "weight_feedback": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev:accum_att_weights"], "n_out": 10},
    "prev_s_state": {"class": "get_last_hidden_state", "from": ["prev:s"], "n_out": 20},
    "prev_s_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev_s_state"], "n_out": 10},
    "energy_in": {"class": "combine", "kind": "add", "from": ["base:enc_ctx", "weight_feedback", "prev_s_transformed"], "n_out": 10},
    "energy_tanh": {"class": "activation", "activation": "tanh", "from": ["energy_in"]},
    "energy": {"class": "linear", "activation": None, "with_bias": False, "from": ["energy_tanh"], "n_out": 1},
    "att_weights": {"class": "softmax_over_spatial", "from": ["energy"]},
    "accum_att_weights": {"class": "eval", "from": ["prev:accum_att_weights", "att_weights", "base:inv_fertility"],
        "eval": "source(0) + source(1) * source(2) * 0.5", "out_type": {"dim": 1, "shape": (None, 1)}},

    "att": {"class": "generic_attention", "weights": "att_weights", "base": "base:encoder"},
    "s": {"class": "rnn_cell", "unit": "LSTMBlock", "from": ["target_embed", "att"], "n_out": 10},
    # Lexicon model
    # Prev output (B, embed_dim) --> (B, intermediary_size)
    "prev_outputs_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev:target_embed"], "n_out": intermediary_size},

    # Prev state (B, hidden) --> (B, intermediary_size)
    "prev_state_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev_s_state"], "n_out": intermediary_size},
    "base_encoder": {"class": "linear", "activation": None, "with_bias": False, "from": ["base:enc_ctx"], "n_out": intermediary_size},

    "prev_prev_state_transformed": {"class": "copy", "from": ["prev:prev_state_transformed"]},

    "output_prob" : {"class": "hmm_factorization", "from": "att_weights",
                     "attention_weights": "att_weights",
                     "base_encoder_transformed": "base_encoder",
                     "prev_state": "prev_state_transformed",
                     "prev_outputs": "prev_outputs_transformed",
		             "top_k": 5,
                     "first_order_alignments": True,
                     "target": "classes", "loss": "ce",
                     "prev_prev_state": "prev_prev_state_transformed",
                     "debug": False,
		             "n_out": num_outputs#['classes'][0]
		             },

}, "target": "classes", "max_seq_len": "max_len_from('base:encoder') * 3", "optimize_move_layers_out": False},

"decision": {
    "class": "decide", "from": ["output"], "loss": "edit_distance", "target": "classes",
    "loss_opts": {
        "debug_print": False
        }
    }
}


search_output_layer = "decision"

debug_print_layer_output_template=True

# trainer
batching = "random"
batch_size = 1000
max_seqs = 1
chunking = "0"
truncation = -1
#gradient_clip = 10
gradient_nan_inf_filter = True
adam = True
gradient_noise = 0.3
learning_rate = 0.0005
learning_rate_control = "newbob"
learning_rate_control_relative_error_relative_lr = True
#model = "./test/transformer-hmm"
model = "/tmp/%s/crnn/%s/model" % (get_login_username(), demo_name)  # https://github.com/tensorflow/tensorflow/issues/6537

num_epochs = 100
save_interval = 1

debug_mode = True
debug_add_check_numerics_ops = True

# log
log_verbosity = 5
